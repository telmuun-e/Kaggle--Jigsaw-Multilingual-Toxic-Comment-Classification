{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import transformers\n",
    "import os\n",
    "import seaborn as sns\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(r\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "train2 = pd.read_csv(r\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
    "valid = pd.read_csv(r\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\")\n",
    "test = pd.read_csv(r\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train1.head())\n",
    "print(train2.head())\n",
    "print(valid.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1[\"toxic\"] = train1[\"toxic\"].round().astype(int)\n",
    "train2[\"toxic\"] = train2[\"toxic\"].round().astype(int)\n",
    "train = pd.concat([\n",
    "    train1.loc[:,[\"id\",\"comment_text\",\"toxic\"]],\n",
    "    train2.loc[:,[\"id\",\"comment_text\",\"toxic\"]]])\n",
    "train.reset_index(drop=True,inplace=True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_len = [len(i.split(\" \")) for i in train.comment_text]\n",
    "sns.distplot(word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text=text.map(lambda x: re.sub(r'\\\\n',' ',str(x)))\n",
    "    text=text.map(lambda x: re.sub(r'[0-9\"]', '', str(x)))\n",
    "    text=text.map(lambda x: re.sub(r'#[\\S]+\\b', '', str(x)))\n",
    "    text=text.map(lambda x: re.sub(r'@[\\S]+\\b', '', str(x)))\n",
    "    text=text.map(lambda x: re.sub(r'https?\\S+', '', str(x)))\n",
    "    text=text.map(lambda x: re.sub(r'\\s+', ' ', str(x)))\n",
    "    text=text.map(lambda x: re.sub(r'\\[\\[User.*','',str(x)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"comment_text\"]=text_cleaning(train[\"comment_text\"])\n",
    "valid[\"comment_text\"]=text_cleaning(valid[\"comment_text\"])\n",
    "test[\"comment_text\"]=text_cleaning(test[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train[\"toxic\"]==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(data,tokenizer,maxlen=512):\n",
    "    encoded_data=tokenizer.batch_encode_plus(text,add_special_tokens=True,\n",
    "                                            return_attention_mask=True,\n",
    "                                            return_token_type_ids=True,\n",
    "                                            pad_to_max_length=True,\n",
    "                                            max_length=maxlen,\n",
    "                                            return_tensors='pt')\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_processing:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.text = data.comment_text.values\n",
    "        self.target = data.toxic.values\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self,index):\n",
    "        encoded = encoding(self.text, self.tokenizer)\n",
    "        ids = encoded[\"input_ids\"]\n",
    "        attention_masks = encoded[\"attention_mask\"]\n",
    "        tokens = encoded[\"token_type_ids\"]\n",
    "        ids, tokens, attention_masks = torch.Tensor(ids), torch.Tensor(tokens), torch.Tensor(attention_mask)\n",
    "        target = torch.Tensor(self.target)\n",
    "        dataset = torch.utils.data.TensorDataset(ids, attention_masks, tokens, target)\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.roberta = transformers.AutoModel.from_pretrained(r\"/kaggle/input/jplu-tf-xlm-roberta-large\")\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(self.roberta.pooler.dense.out_features*2,1)\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, out2 = self.roberta(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        dropped = self.drop(out2)\n",
    "        output = self.out(dropped)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(r\"/kaggle/input/jplu-tf-xlm-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "device = torch.device(\"cuda\")\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=0.001, eps=1e-08)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(trainset)*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(outputs,targets):\n",
    "    output = np.argmax(outputs,axis=1).flatten()\n",
    "    target = targets.flatten()\n",
    "    return roc_auc_score(output, target, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return nn.BCEWithLogitLoss()(outputs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device):\n",
    "    model.train()\n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "        ids = batch['ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token = batch['token_type_ids']\n",
    "        target = batch['target']\n",
    "        ids = ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token = token.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids=ids, \n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids = token)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    predictions, true_vals = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            ids = batch['ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            token = batch['token_type_ids']\n",
    "            target = batch['target']\n",
    "            ids = ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token = token.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input_ids=ids, \n",
    "                         attention_mask=attention_mask,\n",
    "                         token_type_ids = token)\n",
    "            \n",
    "            true_vals.append(target.detach().cpu().numpy())\n",
    "            predictions.append(output.detach().cpu().numpy())\n",
    "            return predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_fn(Data_processing(train, tokenizer), model, optimizer, device)\n",
    "    output, target = eval_fn(Data_processing(valid, tokenizer), model, device)\n",
    "    accuracy = acc_fn(output, target)\n",
    "    print (f\"Epoch: {epoch}, Accuracy: {accuracy}\")\n",
    "    if accuracy > best:\n",
    "        best = accuracy\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
